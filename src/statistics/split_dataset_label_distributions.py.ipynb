{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-11T22:17:01.666150Z"
    }
   },
   "source": [
    "# load corpus dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from src.data.create_corpus_dataset import create_dataset, DatasetConfig, UtteranceType, DatasetSplitType\n",
    "from src.data.dataset_splits import create_splits_from_corpus_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.features.build_features import filter_queries_for_few_shot_setting, filter_passages_for_few_shot_setting\n",
    "\n",
    "# load dataset\n",
    "dataset_folder = \"../../data/processed/with_context\"\n",
    "dataset_path = os.path.join(dataset_folder, \"corpus_dataset_v2\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    # Beispiel zum Erstellen eines Datensatzes. MÃ¶gliche Optionen von DatasetConfig sind im DocString beschrieben.\n",
    "    create_dataset(\n",
    "        DatasetConfig(\n",
    "            dataset_path=dataset_path,\n",
    "            project_dir=\"../../\",\n",
    "            utterance_type=UtteranceType.User,\n",
    "            eval_size=0.5,\n",
    "            validation_test_ratio=0.5\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Beispiel zum Laden des Datensatzes + collate_function des DataLoaders um dynamisch ein Subset der negative passages zu laden.\n",
    "corpus_dataset = load_from_disk(dataset_path)\n",
    "# load split dataset\n",
    "in_distribution_split = create_splits_from_corpus_dataset(corpus_dataset=corpus_dataset,\n",
    "                                                          dataset_split_type=DatasetSplitType.InDistribution,\n",
    "                                                          save_folder=dataset_folder,\n",
    "                                                          dataset_save_name=\"dataset_split_in_distribution\")\n",
    "\n",
    "ids_local = create_splits_from_corpus_dataset(corpus_dataset=corpus_dataset,\n",
    "                                                          dataset_split_type=DatasetSplitType.InDistribution,\n",
    "                                                          save_folder=dataset_folder,\n",
    "                                                          dataset_save_name=\"dataset_split_in_distribution_local\")\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at ../../data/processed/with_context/dataset_split_in_distribution. Loading it.\n",
      "Dataset already exists at ../../data/processed/with_context/dataset_split_in_distribution_local. Loading it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/src/data/dataset_splits.py:112: UserWarning: Overlapping texts between train and test (but no overlapping query ids, so theses are not the same queries): 1\n",
      "Example texts: ['Sicherheit']\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[p for p in corpus_dataset[\"queries\"] if \"NZ.K4-1\" in p[\"labels\"]]",
   "id": "e9ec7130cb00022b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Processing Features\n",
    "num_shots_queries = 1\n",
    "num_shots_passages = 1\n",
    "\n",
    "train_split = in_distribution_split[\"train\"]\n",
    "eval_split = in_distribution_split[\"validation\"]\n",
    "test_split = in_distribution_split[\"test\"]\n",
    "\n",
    "# few-shots queries\n",
    "# train_split = filter_queries_for_few_shot_setting(train_split, num_shots_queries)\n",
    "#\n",
    "#\n",
    "# # few_shots passages\n",
    "# train_split = filter_passages_for_few_shot_setting(train_split, num_shots_passages)\n",
    "\n",
    "\n",
    "# in_distribution_split[\"train\"] = train_split\n",
    "in_distribution_split\n"
   ],
   "id": "66e69c39dea97f7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count labels per scenario and split for queries\n",
    "label_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for query in in_distribution_split[split][\"queries\"]:\n",
    "        scenario = query[\"discussion_scenario\"]\n",
    "        for label in query[\"labels\"]:\n",
    "            label_counts[split][scenario][label] += 1\n",
    "\n",
    "# Build dataframe\n",
    "records = []\n",
    "for split, split_dict in label_counts.items():\n",
    "    for scenario, labels in split_dict.items():\n",
    "        for label, count in labels.items():\n",
    "            records.append({\n",
    "                \"scenario\": scenario,\n",
    "                \"label\": label,\n",
    "                \"split\": split,\n",
    "                \"count\": count\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"simple_split_label_num_queries_distribution.csv\")\n",
    "display(df)\n",
    "\n",
    "# Plot stacked bar chart per scenario\n",
    "scenarios = df[\"scenario\"].unique()\n",
    "for scenario in scenarios:\n",
    "    df_scenario = df[df[\"scenario\"] == scenario]\n",
    "    pivot_df = df_scenario.pivot(index=\"label\", columns=\"split\", values=\"count\").fillna(0).sort_values(\"test\", ascending=False)\n",
    "    display(pivot_df)\n",
    "    pivot_df.to_csv(f\"simple_split_queries_counts_per_label_and_split_{scenario}.csv\")\n",
    "    pivot_df.plot(kind=\"bar\", stacked=True, color=[\"blue\", \"yellow\", \"red\"], figsize=(12, 12))\n",
    "    plt.title(f\"Query  Label Distribution in {scenario} for splits\")\n",
    "    plt.ylabel(\"Query Count\")\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "dfad192a9a4e7cdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_scenario_plots(df, output_dir=\"plots\", show_all_ranks=True, dpi=300, x_label_interval=2):\n",
    "    \"\"\"\n",
    "    Create one rank-frequency plot for each scenario showing all splits as grouped bar charts.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame with columns: scenario, label, split, count\n",
    "    output_dir : str\n",
    "        Directory to save the plots\n",
    "    show_all_ranks : bool\n",
    "        If True, show all ranks; if False, limit to the split with the fewest labels\n",
    "    dpi : int\n",
    "        Resolution for the saved plots\n",
    "    x_label_interval : int\n",
    "        Show x-axis labels every nth rank (2=every second, 3=every third, etc.)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    filenames : list\n",
    "        List of saved filenames\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set plotting style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "    # Colors for splits\n",
    "    split_colors = {\n",
    "        'train': '#4CAF50',      # Green\n",
    "        'validation': '#2196F3', # Blue\n",
    "        'test': '#FF5722'        # Orange\n",
    "    }\n",
    "\n",
    "    # Get all scenarios\n",
    "    scenarios = df['scenario'].unique()\n",
    "\n",
    "    # List to store filenames\n",
    "    filenames = []\n",
    "\n",
    "    # Create one plot per scenario\n",
    "    for scenario in scenarios:\n",
    "        # Filter data for this scenario\n",
    "        scenario_data = df[df['scenario'] == scenario].copy()\n",
    "\n",
    "        # Process each split\n",
    "        split_data = {}\n",
    "        max_rank = 0\n",
    "\n",
    "        for split in ['train', 'validation', 'test']:\n",
    "            # Filter data for this split\n",
    "            split_df = scenario_data[scenario_data['split'] == split].copy()\n",
    "\n",
    "            # Skip if no data\n",
    "            if split_df.empty:\n",
    "                split_data[split] = pd.DataFrame(columns=['rank', 'count', 'label'])\n",
    "                continue\n",
    "\n",
    "            # Sort by count (descending) and add rank\n",
    "            split_df = split_df.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "            split_df['rank'] = split_df.index + 1  # Ranks start at 1\n",
    "\n",
    "            # Store the data\n",
    "            split_data[split] = split_df\n",
    "\n",
    "            # Track the maximum rank for x-axis limit\n",
    "            max_rank = max(max_rank, len(split_df))\n",
    "\n",
    "        # Determine how many ranks to show\n",
    "        if not show_all_ranks:\n",
    "            # Find the minimum number of ranks across all splits with data\n",
    "            non_empty_splits = [len(data) for split, data in split_data.items()\n",
    "                               if not data.empty]\n",
    "            if non_empty_splits:  # Make sure we have at least one non-empty split\n",
    "                max_rank = min(non_empty_splits)\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6), dpi=dpi)\n",
    "\n",
    "        # Set width of bars\n",
    "        bar_width = 0.25\n",
    "\n",
    "        # Create positions for each rank - making sure we start at rank 1\n",
    "        bar_positions = np.arange(max_rank)  # 0-based array\n",
    "\n",
    "        # Create the grouped bar chart\n",
    "        for i, split in enumerate(['train', 'validation', 'test']):\n",
    "            if split_data[split].empty:\n",
    "                continue\n",
    "\n",
    "            # For each split, get the ranks to plot (limited to max_rank)\n",
    "            split_ranks = split_data[split][split_data[split]['rank'] <= max_rank]\n",
    "\n",
    "            if split_ranks.empty:\n",
    "                continue\n",
    "\n",
    "            # Calculate position for this split (adjusting for grouped bars)\n",
    "            # -bar_width, 0, +bar_width for the three splits\n",
    "            position_adjust = (i - 1) * bar_width\n",
    "\n",
    "            # Plot this split's bars\n",
    "            bars = ax.bar(\n",
    "                bar_positions[:len(split_ranks)] + position_adjust + 1,  # +1 to shift to ranks starting at 1\n",
    "                split_ranks['count'].values,\n",
    "                width=bar_width,\n",
    "                color=split_colors[split],\n",
    "                label=f\"{split.capitalize()}\",\n",
    "                alpha=0.8,\n",
    "                edgecolor='black',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "\n",
    "            # Add the label text above each bar\n",
    "            if max_rank <= 15:  # Only add labels if not too crowded\n",
    "                for j, bar in enumerate(bars):\n",
    "                    height = bar.get_height()\n",
    "                    if j < len(split_ranks):  # Make sure we have data for this index\n",
    "                        label_text = split_ranks['label'].iloc[j]\n",
    "                        # Truncate long labels\n",
    "                        if len(label_text) > 10:\n",
    "                            label_text = label_text[:8] + '..'\n",
    "\n",
    "                        # Don't show label for very small bars\n",
    "                        if height > 0.05 * ax.get_ylim()[1]:\n",
    "                            ax.text(\n",
    "                                bar.get_x() + bar.get_width()/2,\n",
    "                                height + 0.01 * ax.get_ylim()[1],\n",
    "                                label_text,\n",
    "                                ha='center',\n",
    "                                va='bottom',\n",
    "                                rotation=90,\n",
    "                                fontsize=8\n",
    "                            )\n",
    "\n",
    "        # Set the x-axis ticks at the center of each group of bars\n",
    "        tick_positions = np.arange(1, max_rank + 1)  # Ranks start at 1\n",
    "        tick_labels = [f\"{r}\" if r % x_label_interval == 1 or r == 1 else \"\"\n",
    "                      for r in range(1, max_rank + 1)]\n",
    "\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Label Rank', fontsize=12)\n",
    "        ax.set_ylabel('Number of Queries', fontsize=12)\n",
    "        # ax.set_title(f'{scenario} - Rank-Frequency Distribution by Split', fontsize=14)\n",
    "\n",
    "        # Add grid and clean up spines\n",
    "        ax.grid(True, linestyle='--', alpha=0.7, axis='y')  # Only horizontal grid lines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # Add legend\n",
    "        ax.legend(frameon=True, framealpha=0.9, loc='upper right')\n",
    "\n",
    "        # Ensure y-axis starts at 0\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "        # Set x-axis limits to ensure proper display of all bars including rank 1\n",
    "        ax.set_xlim(0.5, max_rank + 0.5)\n",
    "\n",
    "        # Add light background to alternate ranks for better readability\n",
    "        # if max_rank > 1:\n",
    "        #     for r in range(0, max_rank, 2):\n",
    "        #         ax.axvspan(\n",
    "        #             r + 0.5,  # Start at the left edge of the rank\n",
    "        #             r + 1.5,  # End at the right edge of the rank\n",
    "        #             color='gray',\n",
    "        #             alpha=0.1\n",
    "        #         )\n",
    "\n",
    "        # Print debug info for first 5 ranks\n",
    "        for split in ['train', 'validation', 'test']:\n",
    "            if not split_data[split].empty and len(split_data[split]) > 0:\n",
    "                first_ranks = split_data[split].head(5)\n",
    "                print(f\"DEBUG - {scenario} - {split} - First 5 ranks:\")\n",
    "                print(first_ranks[['rank', 'count', 'label']])\n",
    "\n",
    "        # Save the plot in EPS format\n",
    "        file_format = \"eps\"\n",
    "        filename = os.path.join(output_dir, f\"{scenario}_rank_frequency.{file_format}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, format=file_format)\n",
    "        plt.close()\n",
    "\n",
    "        filenames.append(filename)\n",
    "\n",
    "    return filenames\n",
    "\n",
    "create_scenario_plots(df, output_dir=\"../../reports/dataset_statistics/corpus_dataset_v2/in_distribution_split\", show_all_ranks=True, dpi=300)"
   ],
   "id": "96e8ceb19d04d8a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count labels per scenario and split for passages\n",
    "label_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for passage in in_distribution_split[split][\"passages\"]:\n",
    "        scenario = passage[\"discussion_scenario\"]\n",
    "        label_counts[split][scenario][passage[\"label\"]] += 1\n",
    "\n",
    "# Build dataframe\n",
    "records = []\n",
    "for split, split_dict in label_counts.items():\n",
    "    for scenario, labels in split_dict.items():\n",
    "        for label, count in labels.items():\n",
    "            records.append({\n",
    "                \"scenario\": scenario,\n",
    "                \"label\": label,\n",
    "                \"split\": split,\n",
    "                \"count\": count\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Plot stacked bar chart per scenario\n",
    "scenarios = df[\"scenario\"].unique()\n",
    "for scenario in scenarios:\n",
    "    df_scenario = df[df[\"scenario\"] == scenario]\n",
    "    pivot_df = df_scenario.pivot(index=\"label\", columns=\"split\", values=\"count\").fillna(0).sort_values(\"test\", ascending=False)\n",
    "    display(pivot_df.tail())\n",
    "    pivot_df.plot(kind=\"bar\", stacked=True, color=[\"blue\", \"yellow\", \"red\"], figsize=(12, 12))\n",
    "    plt.title(f\"Passage Label Distribution in {scenario} for splits\")\n",
    "    plt.ylabel(\"Passage Count\")\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4da7df85a0906173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_label_distribution(df):\n",
    "    \"\"\"\n",
    "    Checks that any (scenario, label) pair found in 'validation' or 'test'\n",
    "    also exists in 'train'. If not, prints out the faulty scenario-label pairs.\n",
    "    Otherwise, confirms that the distribution is valid.\n",
    "    \"\"\"\n",
    "    # Get all (scenario, label) pairs that appear in train (count > 0)\n",
    "    train_pairs = df[(df[\"split\"] == \"train\") & (df[\"count\"] > 0)][[\"scenario\", \"label\"]]\n",
    "    allowed_pairs = set(zip(train_pairs[\"scenario\"], train_pairs[\"label\"]))\n",
    "\n",
    "    # Find all pairs in validation/test that have count > 0\n",
    "    non_train = df[df[\"split\"].isin([\"validation\", \"test\"]) & (df[\"count\"] > 0)].copy()\n",
    "\n",
    "    # Mark which of these are allowed\n",
    "    non_train[\"is_allowed\"] = non_train.apply(\n",
    "        lambda row: (row[\"scenario\"], row[\"label\"]) in allowed_pairs, axis=1\n",
    "    )\n",
    "\n",
    "    # Collect the ones that are not allowed\n",
    "    faulty = non_train[~non_train[\"is_allowed\"]]\n",
    "\n",
    "    if len(faulty) > 0:\n",
    "        print(\"Found scenario/label pairs in test/validation that do not appear in train:\")\n",
    "        print(faulty[[\"scenario\", \"label\", \"split\", \"count\"]])\n",
    "    else:\n",
    "        print(\"All scenario/label pairs in test and validation are valid (they appear in train).\")\n",
    "\n",
    "check_label_distribution(dataframe)\n",
    "\n",
    "\n"
   ],
   "id": "d6fffd753f85b673",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
