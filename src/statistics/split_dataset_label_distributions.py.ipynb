{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T13:39:23.080841Z",
     "start_time": "2025-03-18T13:39:20.484462Z"
    }
   },
   "source": [
    "# load corpus dataset\n",
    "from src.data.create_corpus_dataset import create_dataset, DatasetConfig, UtteranceType, DatasetSplitType\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from src.data.dataset_splits import create_splits_from_corpus_dataset\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "dataset_folder = \"../../data/processed/\"\n",
    "dataset_path = os.path.join(dataset_folder, \"corpus_dataset_v1\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    # Beispiel zum Erstellen eines Datensatzes. Mögliche Optionen von DatasetConfig sind im DocString beschrieben.\n",
    "    create_dataset(\n",
    "        DatasetConfig(\n",
    "            dataset_path=dataset_path,\n",
    "            project_dir=\"../../\",\n",
    "            num_previous_turns=3,\n",
    "            include_role=True,\n",
    "            sep_token=\"[SEP]\",\n",
    "            utterance_type=UtteranceType.User,\n",
    "            eval_size=0.5,\n",
    "            validation_test_ratio=0.5\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Beispiel zum Laden des Datensatzes + collate_function des DataLoaders um dynamisch ein Subset der negative passages zu laden.\n",
    "corpus_dataset = load_from_disk(dataset_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:40:12.651832Z",
     "start_time": "2025-03-18T13:39:23.574537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load split dataset\n",
    "in_distribution_split = create_splits_from_corpus_dataset(corpus_dataset=corpus_dataset,\n",
    "                                                          dataset_split_type=DatasetSplitType.InDistribution,\n",
    "                                                          save_folder=dataset_folder,\n",
    "                                                          dataset_save_name=\"dataset_split_in_distribution_labels_per_scenario\", )\n",
    "in_distribution_split\n",
    "print(\"a\")"
   ],
   "id": "66e69c39dea97f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forced {'id': 117, 'text': 'Stimmt. Ein neuer Rechtsrahmen ist noetig, aber das ist bei jeder grossen neuen Technologie so. Es wird sich lohnen, wenn hierdurch die Diagnosen im Schnitt besser werden.', 'labels': ['Z.K3-1-1-1'], 'discussion_scenario': 'MEDAI'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 72055.22 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 35841.03 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 108954.68 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 83636.71 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 37049.49 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 118589.08 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 85431.66 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 36836.59 examples/s]\n",
      "Filter: 100%|██████████| 2777/2777 [00:00<00:00, 117682.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1943/1943 [00:00<00:00, 116221.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10392/10392 [00:00<00:00, 564104.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1943/1943 [00:00<00:00, 142215.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1943/1943 [00:00<00:00, 140158.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 416/416 [00:00<00:00, 39286.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10392/10392 [00:00<00:00, 525223.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 416/416 [00:00<00:00, 48734.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 416/416 [00:00<00:00, 51642.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 418/418 [00:00<00:00, 45005.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10392/10392 [00:00<00:00, 524231.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 418/418 [00:00<00:00, 49776.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 418/418 [00:00<00:00, 46862.48 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:40:15.173939Z",
     "start_time": "2025-03-18T13:40:15.042426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Count labels per scenario and split\n",
    "label_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for query in in_distribution_split[split][\"queries\"]:\n",
    "        scenario = query[\"discussion_scenario\"]\n",
    "        for label in query[\"labels\"]:\n",
    "            label_counts[split][scenario][label] += 1\n",
    "\n",
    "# Build dataframe\n",
    "records = []\n",
    "for split, split_dict in label_counts.items():\n",
    "    for scenario, labels in split_dict.items():\n",
    "        for label, count in labels.items():\n",
    "            records.append({\n",
    "                \"scenario\": scenario,\n",
    "                \"label\": label,\n",
    "                \"split\": split,\n",
    "                \"count\": count\n",
    "            })\n",
    "\n",
    "dataframe = pd.DataFrame(records)\n",
    "\n",
    "# # Plot stacked bar chart per scenario\n",
    "# scenarios = df[\"scenario\"].unique()\n",
    "# for scenario in scenarios:\n",
    "#     df_scenario = df[df[\"scenario\"] == scenario]\n",
    "#     pivot_df = df_scenario.pivot(index=\"label\", columns=\"split\", values=\"count\").fillna(0)\n",
    "#     pivot_df.plot(kind=\"bar\", stacked=True, color=[\"blue\", \"yellow\", \"red\"], figsize=(12, 6))\n",
    "#     plt.title(f\"Label Distribution in {scenario}\")\n",
    "#     plt.ylabel(\"Count\")\n",
    "#     plt.xlabel(\"Label\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ],
   "id": "dfad192a9a4e7cdd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:40:17.362344Z",
     "start_time": "2025-03-18T13:40:17.353745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_label_distribution(df):\n",
    "    \"\"\"\n",
    "    Checks that any (scenario, label) pair found in 'validation' or 'test'\n",
    "    also exists in 'train'. If not, prints out the faulty scenario-label pairs.\n",
    "    Otherwise, confirms that the distribution is valid.\n",
    "    \"\"\"\n",
    "    # Get all (scenario, label) pairs that appear in train (count > 0)\n",
    "    train_pairs = df[(df[\"split\"] == \"train\") & (df[\"count\"] > 0)][[\"scenario\", \"label\"]]\n",
    "    allowed_pairs = set(zip(train_pairs[\"scenario\"], train_pairs[\"label\"]))\n",
    "\n",
    "    # Find all pairs in validation/test that have count > 0\n",
    "    non_train = df[df[\"split\"].isin([\"validation\", \"test\"]) & (df[\"count\"] > 0)].copy()\n",
    "\n",
    "    # Mark which of these are allowed\n",
    "    non_train[\"is_allowed\"] = non_train.apply(\n",
    "        lambda row: (row[\"scenario\"], row[\"label\"]) in allowed_pairs, axis=1\n",
    "    )\n",
    "\n",
    "    # Collect the ones that are not allowed\n",
    "    faulty = non_train[~non_train[\"is_allowed\"]]\n",
    "\n",
    "    if len(faulty) > 0:\n",
    "        print(\"Found scenario/label pairs in test/validation that do not appear in train:\")\n",
    "        print(faulty[[\"scenario\", \"label\", \"split\", \"count\"]])\n",
    "    else:\n",
    "        print(\"All scenario/label pairs in test and validation are valid (they appear in train).\")\n",
    "\n",
    "check_label_distribution(dataframe)\n",
    "\n",
    "\n"
   ],
   "id": "d6fffd753f85b673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All scenario/label pairs in test and validation are valid (they appear in train).\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
