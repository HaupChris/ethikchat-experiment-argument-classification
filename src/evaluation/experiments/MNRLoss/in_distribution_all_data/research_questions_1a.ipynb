{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:37:22.141540Z",
     "start_time": "2025-05-12T09:37:22.126409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "524148c8b5b06772",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:37:23.606089Z",
     "start_time": "2025-05-12T09:37:23.575545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.evaluation.experiments.MNRLoss.in_distribution_all_data.research_questions_1a import \\\n",
    "    get_folder_and_table_information\n",
    "from src.evaluation.experiments.utils import pandas_df_to_latex\n",
    "\n",
    "pd.set_option('display.width', 1000)  # Increase the total width of the display\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Do not truncate column contents\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "sweep_dir = \"/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/experiments_outputs/6te7vzul/amber-sweep-20\"\n",
    "get_folder_and_table_information(sweep_dir)"
   ],
   "id": "587eec7fbe110b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragrant-sweep-16\n",
      " ├── checkpoint-1925\n",
      " ├── eval\n",
      "  ├── Information-Retrieval_evaluation_eval_results.csv\n",
      "          epoch  steps      loss  cosine-Accuracy@1  cosine-Accuracy@3  cosine-Accuracy@5  cosine-Accuracy@7  cosine-Accuracy@10  cosine-MRR@10  cosine-NDCG@10  loss.1\n",
      "    0  0.498575    175  0.513636           0.720455           0.788636           0.820455           0.836364            0.626124       0.411741             NaN     NaN\n",
      "\n",
      " ├── checkpoint-700\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33253a1f8e797d31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:38:37.390488Z",
     "start_time": "2025-05-12T09:38:37.375290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Set the base directory for results\n",
    "RESULTS_DIR = \"/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/experiments_outputs/6te7vzul\"  # Adjust this to your results directory\n",
    "\n",
    "\n",
    "# List all run directories\n",
    "run_dirs = glob.glob(os.path.join(RESULTS_DIR, \"*\"))\n",
    "\n",
    "run_dirs = [d for d in run_dirs if os.path.isdir(d) and os.path.exists(f\"{d}/accuracy\")]\n",
    "\n",
    "print(f\"Found {len(run_dirs)} experimental runs\")\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 experimental runs\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:40:33.771229Z",
     "start_time": "2025-05-12T09:40:30.753507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.evaluation.experiments.MNRLoss.in_distribution_all_data.research_questions_1a import get_overall_sweep_results\n",
    "from src.evaluation.experiments.utils import pandas_df_to_latex, model_name_to_short_name\n",
    "\n",
    "rename_columns_metrics = {\n",
    "    \"model_name\": \"Model\",\n",
    "    \"topic_token\": \"Topic Token\",\n",
    "    \"context\": \"Context\",\n",
    "    \"lr\": \"LR\",\n",
    "    \"run_dir\": \"run_dir\",\n",
    "    \"Acc@1\": \"Acc@1\",\n",
    "    \"Acc@3\": \"Acc@3\",\n",
    "    \"Acc@5\": \"Acc@5\",\n",
    "    \"Acc@7\": \"Acc@7\",\n",
    "    \"Prec@1\": \"Prec@1\",\n",
    "    \"Prec@3\": \"Prec@3\",\n",
    "    \"Prec@5\": \"Prec@5\",\n",
    "    \"Prec@7\": \"Prec@7\",\n",
    "    \"Rec@1\": \"Rec@1\",\n",
    "    \"Rec@3\": \"Rec@3\",\n",
    "    \"Rec@5\": \"Rec@5\",\n",
    "    \"Rec@7\": \"Rec@7\",\n",
    "}\n",
    "\n",
    "columns_metrics = [\"Model\",\n",
    "                   \"topic_token\",\n",
    "                   \"context\",\n",
    "                   \"Acc@1\",\n",
    "                   \"Acc@3\",\n",
    "                   \"Acc@5\",\n",
    "                   \"Prec@1\",\n",
    "                   \"Prec@3\",\n",
    "                   \"Prec@5\",\n",
    "                   \"Rec@1\",\n",
    "                   \"Rec@3\",\n",
    "                   \"Rec@5\"\n",
    "                   ]\n",
    "\n",
    "results_df = get_overall_sweep_results(run_dirs, model_name_to_short_name)\n",
    "\n",
    "# Sort by accuracy@1 (assuming this is the primary metric of interest)\n",
    "if 'accuracy@1.0' in results_df.columns:\n",
    "    results_df = results_df.sort_values(by='accuracy@1.0', ascending=False)  # Display the top models\n",
    "\n",
    "# Group by model_name and get the run with highest accuracy@1 in each group\n",
    "best_runs = results_df.sort_values('Acc@1', ascending=False)\n",
    "\n",
    "# print metrics table\n",
    "print(\n",
    "    pandas_df_to_latex(best_runs.head(5),\n",
    "                       columns=columns_metrics, rename_columns=rename_columns_metrics\n",
    "                       )\n",
    ")\n",
    "rename_columns_parameters = {\n",
    "    \"model_name\": \"Model\",\n",
    "    \"add_discussion_info\": \"Topic\",\n",
    "    \"context_length\": \"Context\",\n",
    "    \"learning_rate\": \"LR\",\n",
    "    \"batch_size\": \"BS\",\n",
    "    \"num_epochs\": \"Epochs\",\n",
    "    \"warmup_ratio\": \"WR\",\n",
    "}\n",
    "\n",
    "columns_parameters = [ \"Model\", \"Topic\", \"Context\", \"LR\", \"BS\", \"Epochs\", \"WR\"]\n",
    "# print parameters table\n",
    "print(pandas_df_to_latex(best_runs.head(5),\n",
    "                         columns=columns_parameters, rename_columns=rename_columns_parameters,\n",
    "                         float_format=None)\n",
    "      )\n",
    "\n",
    "# Sort overall by accuracy@1 to see which model performed best\n",
    "# best_per_model = best_per_model.sort_values('accuracy@1.0', ascending=False)\n",
    "# best_per_model\n",
    "# print(pandas_df_to_latex(best_per_model))"
   ],
   "id": "fb7e4d8245755b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabularx}{\\textwidth}{CCCCCCCCCCCC}\n",
      "\\toprule\n",
      "Model & Acc@1 & Acc@3 & Acc@5 & Prec@1 & Prec@3 & Prec@5 & Rec@1 & Rec@3 & Rec@5 \\\\\n",
      "\\midrule\n",
      "GBERT Large-aari & 0.63 & 0.74 & 0.79 & 0.63 & 0.63 & 0.63 & 0.62 & 0.73 & 0.78 \\\\\n",
      "GBERT Large-telekom & 0.63 & 0.73 & 0.76 & 0.63 & 0.59 & 0.58 & 0.61 & 0.71 & 0.74 \\\\\n",
      "GBERT Large-aari & 0.62 & 0.72 & 0.78 & 0.62 & 0.61 & 0.61 & 0.61 & 0.71 & 0.76 \\\\\n",
      "GBERT Large-aari & 0.62 & 0.72 & 0.78 & 0.62 & 0.62 & 0.61 & 0.61 & 0.71 & 0.76 \\\\\n",
      "GBERT Large-telekom & 0.62 & 0.73 & 0.76 & 0.62 & 0.61 & 0.61 & 0.61 & 0.71 & 0.74 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabularx}\n",
      "\n",
      "\\begin{tabularx}{\\textwidth}{CCCCCCC}\n",
      "\\toprule\n",
      "Model & Topic & Context & LR & BS & Epochs & WR \\\\\n",
      "\\midrule\n",
      "GBERT Large-aari & True & 2 & 0.000020 & 512 & 12 & 0.100000 \\\\\n",
      "GBERT Large-telekom & False & 1 & 0.000020 & 512 & 12 & 0.100000 \\\\\n",
      "GBERT Large-aari & False & 1 & 0.000030 & 512 & 12 & 0.100000 \\\\\n",
      "GBERT Large-aari & False & 1 & 0.000020 & 512 & 12 & 0.100000 \\\\\n",
      "GBERT Large-telekom & True & 1 & 0.000020 & 512 & 12 & 0.100000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabularx}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:48.835270Z",
     "start_time": "2025-04-29T12:35:48.804622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the top 5 models based on overall accuracy@1\n",
    "top5_models = best_runs.head(5)\n",
    "top5_run_dirs = [os.path.join(RESULTS_DIR, run_dir) if os.path.exists(os.path.join(RESULTS_DIR, run_dir))\n",
    "                 else os.path.join(RESULTS_DIR_2, run_dir)\n",
    "                 for run_dir in top5_models['run_dir']]\n",
    "\n",
    "# Initialize a list to store per-topic results\n",
    "topic_results = []\n",
    "\n",
    "# Process each of the top 5 models\n",
    "for i, run_dir in enumerate(top5_run_dirs):\n",
    "    model_name = top5_models.iloc[i]['model_name']\n",
    "\n",
    "    # Load topic accuracy metrics\n",
    "    topic_acc_path = os.path.join(run_dir, \"accuracy\", \"cosine_topic_accuracy.csv\")\n",
    "\n",
    "    if os.path.exists(topic_acc_path):\n",
    "        topic_df = pd.read_csv(topic_acc_path)\n",
    "\n",
    "        # Process each topic in the dataframe\n",
    "        for _, row in topic_df.iterrows():\n",
    "            topic = row['topic']\n",
    "            accuracy = row['accuracy_at_1']\n",
    "            num_queries = row['num_queries']\n",
    "\n",
    "            topic_results.append({\n",
    "                'model_name': model_name,\n",
    "                'topic': topic,\n",
    "                'accuracy@1': accuracy,\n",
    "                'num_queries': num_queries\n",
    "            })\n",
    "\n",
    "# Create DataFrame from topic results\n",
    "topic_results_df = pd.DataFrame(topic_results)\n",
    "\n",
    "# Pivot the data to have models as columns and topics as rows\n",
    "pivot_df = topic_results_df.pivot_table(\n",
    "    index='topic',\n",
    "    columns='model_name',\n",
    "    values=['accuracy@1', 'num_queries'],\n",
    "    aggfunc='first'  # Take the first value since there should be only one per model-topic pair\n",
    ")\n",
    "\n",
    "# Reshape and clean up the pivot table for better display\n",
    "# This extracts the num_queries only once (they should be the same for all models for a given topic)\n",
    "num_queries_df = topic_results_df[['topic', 'num_queries']].drop_duplicates().set_index('topic')\n",
    "\n",
    "# Create a clean table with just accuracy values\n",
    "accuracy_pivot = pivot_df['accuracy@1']\n",
    "\n",
    "# Add the num_queries column\n",
    "accuracy_pivot['num_queries'] = num_queries_df['num_queries']\n",
    "\n",
    "# Reorder columns to put num_queries last\n",
    "cols = list(accuracy_pivot.columns)\n",
    "cols.remove('num_queries')\n",
    "cols.append('num_queries')\n",
    "accuracy_pivot = accuracy_pivot[cols]\n",
    "\n",
    "# Round accuracy values for cleaner display\n",
    "for col in cols:\n",
    "    if col != 'num_queries':\n",
    "        accuracy_pivot[col] = accuracy_pivot[col].apply(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"N/A\")\n",
    "\n",
    "# Display the resulting table\n",
    "accuracy_pivot.transpose()"
   ],
   "id": "b98b506fcaf3f83c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13168/4249860398.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  accuracy_pivot['num_queries'] = num_queries_df['num_queries']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic                AUTOAI   JURAI   MEDAI   REFAI\n",
       "model_name                                         \n",
       "GBERT Large-aari     0.9437  0.8906  0.9839  0.7600\n",
       "GBERT Large-telekom  0.9296  0.8906  0.9785  0.8000\n",
       "XLMRoberta-EN-DE     0.9225  0.9062  0.9785  0.6400\n",
       "num_queries             142      64     186      25"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic</th>\n",
       "      <th>AUTOAI</th>\n",
       "      <th>JURAI</th>\n",
       "      <th>MEDAI</th>\n",
       "      <th>REFAI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBERT Large-aari</th>\n",
       "      <td>0.9437</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.7600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBERT Large-telekom</th>\n",
       "      <td>0.9296</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLMRoberta-EN-DE</th>\n",
       "      <td>0.9225</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_queries</th>\n",
       "      <td>142</td>\n",
       "      <td>64</td>\n",
       "      <td>186</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:48.886909Z",
     "start_time": "2025-04-29T12:35:48.885416Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f03e1f69e4c6eb5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:49.006666Z",
     "start_time": "2025-04-29T12:35:49.004926Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5155019a6647f9a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:49.069553Z",
     "start_time": "2025-04-29T12:35:49.067885Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1510492edd016e31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:49.125639Z",
     "start_time": "2025-04-29T12:35:49.124029Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d7836e0891f54472",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
