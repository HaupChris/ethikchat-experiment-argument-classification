{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:38.233963Z",
     "start_time": "2025-04-29T12:35:38.222450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "524148c8b5b06772",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:38.989313Z",
     "start_time": "2025-04-29T12:35:38.344067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.evaluation.experiments.MNRLoss.in_distribution_all_data.research_questions_1a import \\\n",
    "    get_folder_and_table_information\n",
    "from src.evaluation.experiments.utils import pandas_df_to_latex\n",
    "\n",
    "pd.set_option('display.width', 1000)  # Increase the total width of the display\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Do not truncate column contents\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "sweep_dir = \"/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/experiments_outputs/e3xgfhuq/amber-sweep-20\"\n",
    "get_folder_and_table_information(sweep_dir)"
   ],
   "id": "587eec7fbe110b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragrant-sweep-16\n",
      " ├── confidence_threshold_metrics\n",
      "  ├── cosine_multi_argument_classification_exact_match.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.045564                       10\n",
      "\n",
      "  ├── cosine_multi_argument_classification_partial_match_noisy.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.031453                      893\n",
      "\n",
      "  ├── cosine_single_argument_classification_merged_queries.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.287528                     1310\n",
      "\n",
      "  ├── cosine_multi_argument_classification_true_partial_match_multi_label.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7       0.0                       10\n",
      "\n",
      "  ├── cosine_multi_argument_classification_partial_match_multi_label.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7       0.0                       10\n",
      "\n",
      "  ├── cosine_single_argument_classification_noisy_queries.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.031453                      893\n",
      "\n",
      "  ├── cosine_single_argument_classification_normal_queries_multi_label.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7       0.5                       10\n",
      "\n",
      "  ├── cosine_multi_argument_classification_true_partial_match.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7       0.0                       10\n",
      "\n",
      "  ├── cosine_single_argument_classification_normal_queries.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.853717                      417\n",
      "\n",
      "  ├── cosine_multi_argument_classification_exact_match_noisy.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.031453                      893\n",
      "\n",
      "  ├── cosine_multi_argument_classification_exact_match_multi_label.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7       0.0                       10\n",
      "\n",
      "  ├── cosine_multi_argument_classification_partial_match.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.045564                       10\n",
      "\n",
      "  ├── cosine_multi_argument_classification_exact_match_merged.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.035848                     1310\n",
      "\n",
      "  ├── cosine_single_argument_classification_normal_queries_single_label.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.862408                      407\n",
      "\n",
      "  ├── cosine_multi_argument_classification_true_partial_match_noisy.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.031453                      893\n",
      "\n",
      "  ├── cosine_multi_argument_classification_exact_match_single_label.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.046683                       10\n",
      "\n",
      "  ├── cosine_multi_argument_classification_true_partial_match_merged.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.021658                     1310\n",
      "\n",
      "  ├── cosine_multi_argument_classification_partial_match_merged.csv\n",
      "       Confidence  Accuracy  Queries_Above_Threshold\n",
      "    0         0.7  0.035848                     1310\n",
      "\n",
      " ├── accuracy\n",
      "  ├── cosine_node_label_accuracy.csv\n",
      "       topic node_label  accuracy_at_1  accuracy_at_3  accuracy_at_5  accuracy_at_7  num_queries  correct_predictions\n",
      "    0  MEDAI       Z.P3            1.0            1.0            1.0            1.0            3                    3\n",
      "\n",
      "  ├── cosine_node_type_accuracy.csv\n",
      "       topic node_type  accuracy_at_1  accuracy_at_3  accuracy_at_5  accuracy_at_7  num_queries  correct_predictions\n",
      "    0  MEDAI         Z       0.870504       0.920863       0.920863       0.920863          139                  121\n",
      "\n",
      "  ├── cosine_node_level_accuracy.csv\n",
      "       topic node_level  accuracy_at_1  accuracy_at_3  accuracy_at_5  accuracy_at_7  num_queries  correct_predictions\n",
      "    0  MEDAI       main        0.87234       0.914894       0.914894       0.914894          141                  123\n",
      "\n",
      "  ├── cosine_topic_accuracy.csv\n",
      "       topic  accuracy_at_1  accuracy_at_3  accuracy_at_5  accuracy_at_7  num_queries  correct_predictions\n",
      "    0  MEDAI       0.903226       0.935484       0.935484       0.935484          186                  168\n",
      "\n",
      " ├── error_analysis_noisy_queries\n",
      "  ├── cosine_error_analysis_noisy_queries.csv\n",
      "      anchor_labels                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              anchor_text  num_anchor_labels anchor_node_type  top1_similarity top1_label                                                           top1_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     top10 first_correct_confidence_threshold\n",
      "    0     ['OTHER']  [MEDAI] [BOT]  Das ist moeglich. Um auch noch einmal eine andere Seite zu beleuchten: Die Anwendung einer medKI ist standardisiert und reproduzierbar. Sie haengt nicht von den unterschiedlichen Faehigkeiten oder der Tagesform der behandelnden Aerztinnen ab. Stattdessen liefert sie gleichbleibend gute Ergebnisse und ermuedet im Gegensatz zum menschlichen Gehirn nicht. [SEP] [USER] werden dann Krankenpfleger besser bezahlt? [SEP] [BOT]  Ich bin mir nicht sicher, was du genau meinst. Kannst du das anders ausdruecken? [SEP] [USER] ok                  1            OTHER         0.827182     Z.K1-2  [MEDAI] Computer urteilen nicht über Leute, das finden die besser.  ['1//Z.K1-2//[MEDAI] Computer urteilen nicht über Leute, das finden die besser.//0.8271817564964294', '2//Z.K1-2//[MEDAI] Patient:innen vertrauen sich lieber einer KI an, da sie nicht verurteilt//0.7196448445320129', '3//Z.K1-2//[MEDAI] Patient:innen empfinden es als ethisch angenehmer, sich einer emotionslosen KI mitzuteilen, die keine impliziten Vorurteile oder moralischen Wertungen besitzt.//0.7076823115348816', '4//Z.K1-2//[MEDAI] Lieber mit Maschine reden, Maschine denkt nicht böse über mich.//0.7054883241653442', '5//Z.K1-2//[MEDAI] Da die KI keine moralischen Urteile fällt, steigt die Akzeptanz medizinischer Empfehlungen durch die Patient:innen deutlich an.//0.7053346633911133', '6//Z.K6-1//[MEDAI] Es ist irrelevant, wie gut die KI ist, wenn wir über Ethik diskutieren.//0.6971561908721924', '7//Z.K1-2//[MEDAI] Die ethische Stärke der KI liegt darin, dass ihre Entscheidungen frei von emotionaler Voreingenommenheit sind und dadurch Patienten sich sicherer fühlen, sensible Themen anzusprechen.//0.6864082217216492', '8//Z.K1-2//[MEDAI] Patienten mögen KI, weil sie sich nicht schlecht fühlen müssen.//0.6857963800430298', '9//Z.K1-2//[MEDAI] Einige Patient:innen vertrauen sich lieber der medKI an, da diese nicht verurteilt und vorurteilsfrei ist. Es ist z.B. leichter das eigene Schamgefühl der KI gegenüber zu überwinden als sich einem Menschen mitzuteilen.//0.6756460666656494', '10//Z.K1-2//[MEDAI] Weil die KI keine Vorurteile hat, ist sie angenehmer für Patienten.//0.6752374172210693']                               0.83\n",
      "\n",
      " ├── eval\n",
      "  ├── Information-Retrieval_evaluation_eval_results.csv\n",
      "         epoch  steps      loss  cosine-Accuracy@1  cosine-Accuracy@3  cosine-Accuracy@5  cosine-MRR@10  cosine-NDCG@10  loss.1\n",
      "    0  0.49922    320  0.515663           0.684337           0.739759           0.613658        0.44977             NaN     NaN\n",
      "\n",
      " ├── checkpoint-5120\n",
      " ├── error_analysis\n",
      "  ├── cosine_error_analysis.csv\n",
      "      anchor_labels                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    anchor_text  num_anchor_labels anchor_node_type  top1_similarity top1_label                  top1_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            top10  top1_prediction_correct  rank_first_relevant\n",
      "    0      ['Z.P3']  [MEDAI] [BOT]  Ich bin mir nicht sicher, was du genau meinst. Kannst du das anders ausdruecken? [SEP] [USER] Wie soll die Krankenkasse eine medKI Diagnose abrechnen?  [SEP] [BOT]  Das mag sein. Aber hast Du schon einmal daran gedacht: Wenn Fehler vom Fachpersonal in den Datenerhebungszentren gemacht werden, kommt die medKI zu falschen Schlussfolgerungen. Daher kann man sich auf die Entscheidungen nicht verlassen. [SEP] [USER] Das Wissen einer KI kann eine deutlich hoehere Menge erreichen, als das Wissen eines Menschen.                   1             Main         0.983351       Z.P3  [MEDAI] KIs sind schlauer  ['1//Z.P3//[MEDAI] KIs sind schlauer//0.9833506345748901', '2//Z.P3//[MEDAI] Die KI haette Zugriff auf die neusten Erkenntnisse der Wissenschaft. Aerztinnen bilden sich oft nicht konstant weiter und haben irgendwann veraltetes Wissen //0.9817087650299072', '3//Z.P3//[MEDAI]  Eine KI hat bessere Moeglichkeiten an Informattionen zu kommen //0.9815599918365479', '4//Z.P3//[MEDAI] medKI weiß immer alles.//0.981359601020813', '5//Z.P3//[MEDAI] Wenn die KI gut ausgebildet ist, steht ihr Wissen der medizinischen Fachexpertise von Aerzten in nichts nach.//0.9812694787979126', '6//Z.P3//[MEDAI] Dennoch muss es doch einfacher sein, die KI auf dem aktuellen Stand zu halten, als hunderttausende von Aerzten zur Weiterbildung zu motivieren //0.9807603359222412', '7//Z.P3//[MEDAI] Eine KI kann mehr Wissen als ein Mensch//0.9804705381393433', '8//Z.P3//[MEDAI] KI verfuegt ueber breitflaechiges wissen auch was ausnahmen betrifft //0.9765448570251465', '9//Z.P3//[MEDAI] medKI garantiert Patient:innen den Zugang zu den neuesten medizinischen Erkenntnissen.//0.9761879444122314', '10//Z.P3//[MEDAI] Patient:innen haben ein Recht auf die neuesten evidenzbasierten Empfehlungen hinsichtlich ihrer Diagnostik und Therapie, was durch eine medKI gewährleistet ist. Das ist bei Ärzt:innen, die sich neben ihrem stressigen Job mühsam weiterbilden müssen, nicht immer gewährleistet.//0.976138174533844']                     True                    1\n",
      "\n",
      " ├── noisy_query_analysis\n",
      "  ├── noisy_query_type_distribution.csv\n",
      "      Noisy Query Type  Count Percentage                                                                                                                                                                                                                                                                                                              Example Texts\n",
      "    0          CONSENT    162     17.57%  [MEDAI] [BOT]  Das ist moeglich. Ansprechen moechte ich noch: Die Aus- und Weiterbildung von Aerztin...\\n[MEDAI] [BOT]  Das ist moeglich. Ansprechen moechte ich noch: Die Aus- und Weiterbildung von Aerztin...\\n[MEDAI] [BOT]  Ich bin mir nicht sicher, was du genau meinst. Kannst du das anders ausdruecken? [SEP...\n",
      "\n",
      " ├── noisy_query_performance\n",
      "  ├── cosine_noisy_type_fp_rates.csv\n",
      "      Noisy Query Type  Confidence Threshold  False Positive Rate  False Positives  Number of Queries\n",
      "    0          CONSENT                   0.7               0.9877              160                162\n",
      "\n",
      "  ├── cosine_noisy_type_accuracy.csv\n",
      "      Noisy Query Type  K  Confidence Threshold  Accuracy@K  Correct Predictions  Number of Queries\n",
      "    0          CONSENT  1                   0.7      0.0123                    2                162\n",
      "\n",
      " ├── checkpoint-4160\n",
      " ├── overall_metrics\n",
      "  ├── cosine_overall_precision.csv\n",
      "       Metric     Value\n",
      "    0       1  0.853717\n",
      "\n",
      "  ├── cosine_overall_accuracy.csv\n",
      "       Metric     Value\n",
      "    0       1  0.853717\n",
      "\n",
      " ├── precision\n",
      "  ├── cosine_node_type_precision.csv\n",
      "       topic node_type  precision_at_1  precision_at_3  precision_at_5  precision_at_7\n",
      "    0  MEDAI         Z        0.870504        0.899281        0.893525        0.892086\n",
      "\n",
      "  ├── cosine_node_label_precision.csv\n",
      "       topic node_label  precision_at_1  precision_at_3  precision_at_5  precision_at_7\n",
      "    0  MEDAI       Z.P3             1.0             1.0             1.0             1.0\n",
      "\n",
      "  ├── cosine_topic_precision.csv\n",
      "       topic  precision_at_1  precision_at_3  precision_at_5  precision_at_7\n",
      "    0  MEDAI        0.903226        0.917563        0.910753        0.907834\n",
      "\n",
      "  ├── cosine_node_level_precision.csv\n",
      "       topic node_level  precision_at_1  precision_at_3  precision_at_5  precision_at_7\n",
      "    0  MEDAI       main         0.87234        0.898345        0.893617        0.895643\n",
      "\n",
      " ├── run_config.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:39.171304Z",
     "start_time": "2025-04-29T12:35:39.158239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Set the base directory for results\n",
    "RESULTS_DIR = \"/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/experiments_outputs/5qgtbfrb\"  # Adjust this to your results directory\n",
    "\n",
    "RESULTS_DIR_2 = \"/home/christian/PycharmProjects/ethikchat-experiment-argument-classification/experiments_outputs/e3xgfhuq\"\n",
    "\n",
    "# List all run directories\n",
    "run_dirs = glob.glob(os.path.join(RESULTS_DIR, \"*\"))\n",
    "run_dirs_2 = glob.glob(os.path.join(RESULTS_DIR_2, \"*\"))\n",
    "\n",
    "run_dirs = [d for d in run_dirs if os.path.isdir(d)]\n",
    "run_dirs_2 = [d for d in run_dirs_2 if os.path.isdir(d)]\n",
    "\n",
    "run_dirs.extend(run_dirs_2)\n",
    "print(f\"Found {len(run_dirs)} experimental runs\")\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79 experimental runs\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:49:43.666504Z",
     "start_time": "2025-04-29T12:49:33.918337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.evaluation.experiments.MNRLoss.in_distribution_all_data.research_questions_1a import get_overall_sweep_results\n",
    "from src.evaluation.experiments.utils import pandas_df_to_latex, model_name_to_short_name\n",
    "\n",
    "rename_columns_metrics = {\n",
    "    \"model_name\": \"Model\",\n",
    "    \"topic_token\": \"Topic Token\",\n",
    "    \"context\": \"Context\",\n",
    "    \"lr\": \"LR\",\n",
    "    \"run_dir\": \"run_dir\",\n",
    "    \"Acc@1\": \"Acc@1\",\n",
    "    \"Acc@3\": \"Acc@3\",\n",
    "    \"Acc@5\": \"Acc@5\",\n",
    "    \"Acc@7\": \"Acc@7\",\n",
    "    \"Prec@1\": \"Prec@1\",\n",
    "    \"Prec@3\": \"Prec@3\",\n",
    "    \"Prec@5\": \"Prec@5\",\n",
    "    \"Prec@7\": \"Prec@7\",\n",
    "    \"Rec@1\": \"Rec@1\",\n",
    "    \"Rec@3\": \"Rec@3\",\n",
    "    \"Rec@5\": \"Rec@5\",\n",
    "    \"Rec@7\": \"Rec@7\",\n",
    "}\n",
    "\n",
    "columns_metrics = [\"Model\",\n",
    "                   \"topic_token\",\n",
    "                   \"context\",\n",
    "                   \"Acc@1\",\n",
    "                   \"Acc@3\",\n",
    "                   \"Acc@5\",\n",
    "                   \"Prec@1\",\n",
    "                   \"Prec@3\",\n",
    "                   \"Prec@5\",\n",
    "                   \"Rec@1\",\n",
    "                   \"Rec@3\",\n",
    "                   \"Rec@5\"\n",
    "                   ]\n",
    "\n",
    "results_df = get_overall_sweep_results(run_dirs, model_name_to_short_name)\n",
    "\n",
    "# Sort by accuracy@1 (assuming this is the primary metric of interest)\n",
    "if 'accuracy@1.0' in results_df.columns:\n",
    "    results_df = results_df.sort_values(by='accuracy@1.0', ascending=False)  # Display the top models\n",
    "\n",
    "# Group by model_name and get the run with highest accuracy@1 in each group\n",
    "best_runs = results_df.sort_values('Acc@1', ascending=False)\n",
    "\n",
    "# print metrics table\n",
    "print(\n",
    "    pandas_df_to_latex(best_runs.head(5),\n",
    "                       columns=columns_metrics, rename_columns=rename_columns_metrics\n",
    "                       )\n",
    ")\n",
    "rename_columns_parameters = {\n",
    "    \"model_name\": \"Model\",\n",
    "    \"add_discussion_info\": \"Topic\",\n",
    "    \"context_length\": \"Context\",\n",
    "    \"learning_rate\": \"LR\",\n",
    "    \"batch_size\": \"BS\",\n",
    "    \"num_epochs\": \"Epochs\",\n",
    "    \"warmup_ratio\": \"WR\",\n",
    "}\n",
    "\n",
    "columns_parameters = [ \"Model\", \"Topic\", \"Context\", \"LR\", \"BS\", \"Epochs\", \"WR\"]\n",
    "# print parameters table\n",
    "print(pandas_df_to_latex(best_runs.head(5),\n",
    "                         columns=columns_parameters, rename_columns=rename_columns_parameters,\n",
    "                         float_format=None)\n",
    "      )\n",
    "\n",
    "# Sort overall by accuracy@1 to see which model performed best\n",
    "# best_per_model = best_per_model.sort_values('accuracy@1.0', ascending=False)\n",
    "# best_per_model\n",
    "# print(pandas_df_to_latex(best_per_model))"
   ],
   "id": "fb7e4d8245755b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabularx}{\\textwidth}{CCCCCCCCCCCC}\n",
      "\\toprule\n",
      "Model & Acc@1 & Acc@3 & Acc@5 & Prec@1 & Prec@3 & Prec@5 & Rec@1 & Rec@3 & Rec@5 \\\\\n",
      "\\midrule\n",
      "GBERT Large-aari & 0.94 & 0.98 & 0.98 & 0.94 & 0.95 & 0.95 & 0.93 & 0.97 & 0.97 \\\\\n",
      "GBERT Large-telekom & 0.94 & 0.95 & 0.96 & 0.94 & 0.93 & 0.93 & 0.93 & 0.95 & 0.95 \\\\\n",
      "GBERT Large-telekom & 0.93 & 0.96 & 0.97 & 0.93 & 0.93 & 0.92 & 0.93 & 0.95 & 0.96 \\\\\n",
      "XLMRoberta-EN-DE & 0.93 & 0.95 & 0.96 & 0.93 & 0.91 & 0.91 & 0.92 & 0.94 & 0.96 \\\\\n",
      "GBERT Large-aari & 0.92 & 0.96 & 0.96 & 0.92 & 0.91 & 0.91 & 0.91 & 0.95 & 0.96 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabularx}\n",
      "\n",
      "\\begin{tabularx}{\\textwidth}{CCCCCCC}\n",
      "\\toprule\n",
      "Model & Topic & Context & LR & BS & Epochs & WR \\\\\n",
      "\\midrule\n",
      "GBERT Large-aari & False & 0 & 0.000030 & 256 & 10 & 0.150000 \\\\\n",
      "GBERT Large-telekom & True & 0 & 0.000020 & 256 & 8 & 0.100000 \\\\\n",
      "GBERT Large-telekom & False & 0 & 0.000020 & 256 & 10 & 0.100000 \\\\\n",
      "XLMRoberta-EN-DE & False & 0 & 0.000020 & 128 & 12 & 0.150000 \\\\\n",
      "GBERT Large-aari & False & 0 & 0.000020 & 128 & 12 & 0.150000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabularx}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:48.835270Z",
     "start_time": "2025-04-29T12:35:48.804622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the top 5 models based on overall accuracy@1\n",
    "top5_models = best_runs.head(5)\n",
    "top5_run_dirs = [os.path.join(RESULTS_DIR, run_dir) if os.path.exists(os.path.join(RESULTS_DIR, run_dir))\n",
    "                 else os.path.join(RESULTS_DIR_2, run_dir)\n",
    "                 for run_dir in top5_models['run_dir']]\n",
    "\n",
    "# Initialize a list to store per-topic results\n",
    "topic_results = []\n",
    "\n",
    "# Process each of the top 5 models\n",
    "for i, run_dir in enumerate(top5_run_dirs):\n",
    "    model_name = top5_models.iloc[i]['model_name']\n",
    "\n",
    "    # Load topic accuracy metrics\n",
    "    topic_acc_path = os.path.join(run_dir, \"accuracy\", \"cosine_topic_accuracy.csv\")\n",
    "\n",
    "    if os.path.exists(topic_acc_path):\n",
    "        topic_df = pd.read_csv(topic_acc_path)\n",
    "\n",
    "        # Process each topic in the dataframe\n",
    "        for _, row in topic_df.iterrows():\n",
    "            topic = row['topic']\n",
    "            accuracy = row['accuracy_at_1']\n",
    "            num_queries = row['num_queries']\n",
    "\n",
    "            topic_results.append({\n",
    "                'model_name': model_name,\n",
    "                'topic': topic,\n",
    "                'accuracy@1': accuracy,\n",
    "                'num_queries': num_queries\n",
    "            })\n",
    "\n",
    "# Create DataFrame from topic results\n",
    "topic_results_df = pd.DataFrame(topic_results)\n",
    "\n",
    "# Pivot the data to have models as columns and topics as rows\n",
    "pivot_df = topic_results_df.pivot_table(\n",
    "    index='topic',\n",
    "    columns='model_name',\n",
    "    values=['accuracy@1', 'num_queries'],\n",
    "    aggfunc='first'  # Take the first value since there should be only one per model-topic pair\n",
    ")\n",
    "\n",
    "# Reshape and clean up the pivot table for better display\n",
    "# This extracts the num_queries only once (they should be the same for all models for a given topic)\n",
    "num_queries_df = topic_results_df[['topic', 'num_queries']].drop_duplicates().set_index('topic')\n",
    "\n",
    "# Create a clean table with just accuracy values\n",
    "accuracy_pivot = pivot_df['accuracy@1']\n",
    "\n",
    "# Add the num_queries column\n",
    "accuracy_pivot['num_queries'] = num_queries_df['num_queries']\n",
    "\n",
    "# Reorder columns to put num_queries last\n",
    "cols = list(accuracy_pivot.columns)\n",
    "cols.remove('num_queries')\n",
    "cols.append('num_queries')\n",
    "accuracy_pivot = accuracy_pivot[cols]\n",
    "\n",
    "# Round accuracy values for cleaner display\n",
    "for col in cols:\n",
    "    if col != 'num_queries':\n",
    "        accuracy_pivot[col] = accuracy_pivot[col].apply(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"N/A\")\n",
    "\n",
    "# Display the resulting table\n",
    "accuracy_pivot.transpose()"
   ],
   "id": "b98b506fcaf3f83c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13168/4249860398.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  accuracy_pivot['num_queries'] = num_queries_df['num_queries']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic                AUTOAI   JURAI   MEDAI   REFAI\n",
       "model_name                                         \n",
       "GBERT Large-aari     0.9437  0.8906  0.9839  0.7600\n",
       "GBERT Large-telekom  0.9296  0.8906  0.9785  0.8000\n",
       "XLMRoberta-EN-DE     0.9225  0.9062  0.9785  0.6400\n",
       "num_queries             142      64     186      25"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic</th>\n",
       "      <th>AUTOAI</th>\n",
       "      <th>JURAI</th>\n",
       "      <th>MEDAI</th>\n",
       "      <th>REFAI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBERT Large-aari</th>\n",
       "      <td>0.9437</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.7600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBERT Large-telekom</th>\n",
       "      <td>0.9296</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLMRoberta-EN-DE</th>\n",
       "      <td>0.9225</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_queries</th>\n",
       "      <td>142</td>\n",
       "      <td>64</td>\n",
       "      <td>186</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:48.886909Z",
     "start_time": "2025-04-29T12:35:48.885416Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f03e1f69e4c6eb5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:49.006666Z",
     "start_time": "2025-04-29T12:35:49.004926Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5155019a6647f9a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:49.069553Z",
     "start_time": "2025-04-29T12:35:49.067885Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1510492edd016e31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:35:49.125639Z",
     "start_time": "2025-04-29T12:35:49.124029Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d7836e0891f54472",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
